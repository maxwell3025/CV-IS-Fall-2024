{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated-gradient on IMDB dataset (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of the integrated-gradient method on text classification with a PyTorch model. If using this explainer, please cite the original work: https://github.com/ankurtaly/Integrated-Gradients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:27:27.249040Z",
     "start_time": "2024-11-07T19:27:27.245856Z"
    }
   },
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from omnixai.data.text import Text\n",
    "from omnixai.preprocessing.text import Word2Id\n",
    "from omnixai.explainers.tabular.agnostic.L2X.utils import Trainer, InputData, DataLoader\n",
    "from omnixai.explainers.nlp.specific.ig import IntegratedGradientText"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a simple CNN model for this text classification task. Note that the method `forward` has two inputs `inputs` (token ids) and `masks` (the sentence masks). For `IntegratedGradientText`, the first input of the model must be the token ids."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:27:31.112508Z",
     "start_time": "2024-11-07T19:27:31.105965Z"
    }
   },
   "source": [
    "class TextModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, num_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_size = kwargs.get(\"embedding_size\", 50)\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_size)\n",
    "        self.embedding.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        \n",
    "        hidden_size = kwargs.get(\"hidden_size\", 100)\n",
    "        kernel_sizes = kwargs.get(\"kernel_sizes\", [3, 4, 5])\n",
    "        if type(kernel_sizes) == int:\n",
    "            kernel_sizes = [kernel_sizes]\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(self.embedding_size, hidden_size, k, padding=k // 2) for k in kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.output_layer = nn.Linear(len(kernel_sizes) * hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        x = embeddings * masks.unsqueeze(dim=-1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = [self.activation(layer(x).max(2)[0]) for layer in self.conv_layers]\n",
    "        outputs = self.output_layer(self.dropout(torch.cat(x, dim=1)))\n",
    "        if outputs.shape[1] == 1:\n",
    "            outputs = outputs.squeeze(dim=1)\n",
    "        return outputs"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Text` object to represent a batch of texts/sentences. The package `omnixai.preprocessing.text` provides some transforms related to text data such as `Tfidf` and `Word2Id`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:27:59.541566Z",
     "start_time": "2024-11-07T19:27:32.587151Z"
    }
   },
   "source": [
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('../data/imdb.csv')\n",
    "n = int(0.8 * len(train_data))\n",
    "x_train = Text(train_data[\"review\"].values[:n])\n",
    "y_train = train_data[\"sentiment\"].values[:n].astype(int)\n",
    "x_test = Text(train_data[\"review\"].values[n:])\n",
    "y_test = train_data[\"sentiment\"].values[n:].astype(int)\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "# The transform for converting words/tokens to IDs\n",
    "transform = Word2Id().fit(x_train)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing function converts a batch of texts into token IDs and the masks. The outputs of the preprocessing function must fit the inputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:28:01.543309Z",
     "start_time": "2024-11-07T19:28:01.540668Z"
    }
   },
   "source": [
    "max_length = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def preprocess(X: Text):\n",
    "    samples = transform.transform(X)\n",
    "    max_len = 0\n",
    "    for i in range(len(samples)):\n",
    "        max_len = max(max_len, len(samples[i]))\n",
    "    max_len = min(max_len, max_length)\n",
    "    inputs = np.zeros((len(samples), max_len), dtype=int)\n",
    "    masks = np.zeros((len(samples), max_len), dtype=np.float32)\n",
    "    for i in range(len(samples)):\n",
    "        x = samples[i][:max_len]\n",
    "        inputs[i, :len(x)] = x\n",
    "        masks[i, :len(x)] = 1\n",
    "    return inputs, masks"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the CNN model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:56:44.848493Z",
     "start_time": "2024-11-07T20:54:37.964122Z"
    }
   },
   "source": [
    "model = TextModel(\n",
    "    num_embeddings=transform.vocab_size,\n",
    "    num_classes=len(class_names)\n",
    ").to(device)\n",
    "try:\n",
    "    path = './model.pth'\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    print(\"loaded\")\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "\n",
    "Trainer(\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=128,\n",
    "    num_epochs=1,\n",
    ").train(\n",
    "    model=model,\n",
    "    loss_func=nn.CrossEntropyLoss(),\n",
    "    train_x=transform.transform(x_train),\n",
    "    train_y=y_train,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "path = './model.pth'\n",
    "torch.save(model.state_dict(), path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      " |████████████████████████████████████████| 100.0% Complete, Loss 0.0080\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:24:56.745318Z",
     "start_time": "2024-11-07T20:24:16.946405Z"
    }
   },
   "source": [
    "model.eval()\n",
    "data = transform.transform(x_test)\n",
    "data_loader = DataLoader(\n",
    "    dataset=InputData(data, [0] * len(data), max_length),\n",
    "    batch_size=32,\n",
    "    collate_fn=InputData.collate_func,\n",
    "    shuffle=False\n",
    ")\n",
    "outputs = []\n",
    "for inputs in data_loader:\n",
    "    value, mask, target = inputs\n",
    "    y = model(value.to(device), mask.to(device))\n",
    "    outputs.append(y.detach().cpu().numpy())\n",
    "outputs = np.concatenate(outputs, axis=0)\n",
    "predictions = np.argmax(outputs, axis=1)\n",
    "print('Test accuracy: {}'.format(\n",
    "    sklearn.metrics.f1_score(y_test, predictions, average='binary')))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8854501216545012\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize `IntegratedGradientText`, we need to set the following parameters:\n",
    "\n",
    "  - `model`: The model to explain, whose type is `tf.keras.Model` or `torch.nn.Module`.\n",
    "  - `embedding_layer`: The embedding layer in the model, which can be `tf.keras.layers.Layer` or `torch.nn.Module`.\n",
    "  - `preprocess_function`: The pre-processing function that converts the raw input data into the inputs of `model`. The first output of `preprocess_function` should be the token ids.\n",
    "  - `mode`: The task type, e.g., `classification` or `regression`.\n",
    "  - `id2token`: The mapping from token ids to tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:52:29.981152Z",
     "start_time": "2024-11-07T20:52:29.695383Z"
    }
   },
   "source": [
    "allwords = \"\"\n",
    "explainer = IntegratedGradientText(\n",
    "    model=model,\n",
    "    embedding_layer=model.embedding,\n",
    "    preprocess_function=preprocess,\n",
    "    id2token=transform.id_to_word\n",
    ")\n",
    "x = Text([\n",
    "\"This movie was the cinematic equivalent of watching paint dry.\",\n",
    "\"I laughed, I cried, I fell asleep - what an emotional rollercoaster!\",\n",
    "\"The plot twists in this film were more predictable than a daytime soap opera.\",\n",
    "\"Despite its flaws, there was something oddly charming about this movie's quirky protagonist.\",\n",
    "\"Imagine taking the worst elements of every genre and combining them into one incoherent mess. That's this film in a nutshell.\",\n",
    "\"I'm honestly not sure if I loved or hated this movie - it was that perplexingly strange.\",\n",
    "\"The special effects were so breathtakingly realistic, I forgot I was watching a work of fiction.\",\n",
    "\"I'd rather have root canal surgery than sit through this movie again.\",\n",
    "\"A solid 'meh' from me. Completely forgettable, but not actively unenjoyable either.\",\n",
    "\"This movie was the cinematic equivalent of a warm cup of milk - harmless, but not particularly exciting.\",\n",
    "])\n",
    "rawExplanations = explainer.explain(x)\n",
    "explanations = ast.literal_eval(str(rawExplanations))\n",
    "result = []\n",
    "for i in range(len(explanations)):\n",
    "    tokens = explanations[i][\"tokens\"]\n",
    "    scores = explanations[i][\"scores\"]\n",
    "    append = []\n",
    "    for j in range(len(tokens)):\n",
    "        word = tokens[j]\n",
    "        score = scores[j]\n",
    "        if (int(explanations[i][\"target_label\"] == 0)):\n",
    "            score *= float(-1)\n",
    "        append.append([word, score])\n",
    "    result.append(append)\n",
    "totals = []\n",
    "for i in range(len(result)):\n",
    "    wordScores = result[i]\n",
    "    total = 0\n",
    "    for j in range(len(wordScores)):\n",
    "        total += wordScores[j][1]\n",
    "    totals.append(total)\n",
    "result.append(totals)\n",
    "print(result)\n",
    "# print(sorted(result, key=lambda x: x[1]))\n",
    "rawExplanations.ipython_plot(class_names=class_names)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['this', 0.24439932455804175], ['movie', 0.05638953305392258], ['was', -0.11364241896166592], ['the', 0.0734531548248518], ['cinematic', -0.05027196220877296], ['equivalent', -0.006102904416397761], ['of', 0.34827290113764003], ['watching', -0.2484394243985918], ['paint', -0.9119018231743818], ['dry', -1.1809996579923794]], [['i', -0.06134133863345417], ['laughed', 1.3757163551648801], ['i', 0.3416740493392372], ['cried', 4.314692952638511], ['i', 0.23247546696249063], ['fell', -2.8439235097262845], ['asleep', -2.367692449704707], ['what', 0.47426367554437415], ['an', 0.17344284351690134], ['emotional', 0.11976299364132414], ['rollercoaster', 0.00015693529280373653]], [['the', -0.0716057867043087], ['plot', -0.11605594109757189], ['twists', 0.26913418374024295], ['in', -0.12094787241463391], ['this', -0.04835812672748237], ['film', -0.0021218357937717175], ['were', 0.19803726134979066], ['more', 0.0573806224061085], ['predictable', -3.9000199498748174], ['than', -0.508941054289978], ['a', 0.28180106515555264], ['daytime', 0.13400231678496075], ['soap', -0.3410818152340534], ['opera', -0.06011598908821532]], [['despite', -0.04251463143680313], ['its', 0.14168848249519211], ['flaws', -0.013981627096615792], ['there', -0.06921900832286407], ['was', -0.09508417398988239], ['something', -0.10070137207084288], ['oddly', -0.002470834571888963], ['charming', 0.8239578121720761], ['about', -0.303771165535666], ['this', 0.49489396293162924], ['movie', 0.027235212830929778], ['quirky', 1.026023759162932], ['protagonist', -0.00017059059785275166]], [['imagine', 0.007251781910143692], ['taking', 0.2354702004999031], ['the', -0.03738284565718895], ['worst', -0.37370041194334175], ['elements', 0.03829172068142691], ['of', 0.01909429701788883], ['every', 0.03416835453380222], ['genre', -0.032671349602402124], ['and', -0.010175950590506208], ['combining', -0.07584443255532683], ['them', 0.5325506932677272], ['into', -0.02622386984221743], ['one', 0.2572412272296628], ['incoherent', -3.9486378725877453], ['mess', -2.662959594269509], ['that', 0.495249519385798], ['this', 0.15117824011518805], ['film', 0.0165590088100485], ['in', -0.046064369901699265], ['a', 0.026966473719243635], ['nutshell', 0.05377634509713218]], [['i', -0.03397338835159128], ['honestly', -2.006279240282935], ['not', -0.7664605678607814], ['sure', 0.2505167416674484], ['if', -0.03560123444408838], ['i', 0.10625966437467463], ['loved', 0.045910651088309926], ['or', -0.20540441149267397], ['hated', -0.49458918695781484], ['this', -0.23138081489353882], ['movie', -0.06469415365726926], ['it', 0.23086747175989056], ['was', 0.1443455668206489], ['that', 0.007493347465151333], ['<UNK>', -0.005017164328766297], ['strange', 0.07401487188330569]], [['the', -0.02053972693680553], ['special', -0.08436361808452182], ['effects', -0.21277392339613024], ['were', -0.4362910082496378], ['so', 0.08940408798004323], ['breathtakingly', 6.218636599430004], ['realistic', 2.6368932764772834], ['i', -0.2120620394247196], ['forgot', -0.6080929631971943], ['i', 0.013384773604586953], ['was', -0.04034837039075116], ['watching', -0.06727680387728366], ['a', -0.01998373027962854], ['work', -0.012382628852397048], ['of', 0.0016585640158906932], ['fiction', -0.009443259343431524]], [['i', 0.009947026265653709], ['rather', 0.23675175162018824], ['have', 0.23077824666230096], ['root', -1.2735489420258277], ['canal', -0.980666605362077], ['surgery', -0.437273861745652], ['than', -0.27862832492600464], ['sit', 0.3825395894396238], ['through', -0.28499166169026474], ['this', -0.336308737510993], ['movie', 0.11937304183196827], ['again', 0.3592816399282678]], [['a', 0.28881002150534124], ['solid', 2.2064165113168315], ['from', 1.0035111589213717], ['me', 0.7891640613299942], ['completely', -0.9360813486090105], ['forgettable', -3.448844664412455], ['but', 1.4195175157749353], ['not', -0.017181886542224745], ['actively', 0.15874368545413123], ['unenjoyable', -0.15302770866211646], ['either', -0.38705852710515065]], [['this', 0.0195035471655349], ['movie', -0.1659804138816484], ['was', -0.3004326577953304], ['the', 0.3082471572036377], ['cinematic', -0.059750113302706305], ['equivalent', -0.08344569904553872], ['of', 0.07976009493979777], ['a', 0.0010068636989490898], ['warm', 2.2274536788804165], ['cup', 0.12018116421364844], ['of', -0.38352066684836955], ['milk', 0.4469866888394067], ['harmless', 0.48489451321317545], ['but', 0.047536357472337504], ['not', -0.1850132044728772], ['particularly', 0.05221711368586728], ['exciting', -0.009914146619073224]], [-1.7888432775777336, 1.7592279740360768, -4.228892921788177, 1.885885825970343, -5.345862834681972, -2.9839918472100297, 7.236419229475307, -2.252746837512816, 0.9239688189716474, 2.5997302773472275]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div>Instance 0: Class negative</div>\n",
       "<div><span style='color:rgb(153,115,115)'>this </span><span style='color:rgb(133,125,125)'>movie </span><span style='color:rgb(122,139,122)'>was </span><span style='color:rgb(134,125,125)'>the </span><span style='color:rgb(126,132,126)'>cinematic </span><span style='color:rgb(128,127,128)'>equivalent </span><span style='color:rgb(164,110,110)'>of </span><span style='color:rgb(115,153,115)'>watching </span><span style='color:rgb(79,225,79)'>paint </span><span style='color:rgb(65,254,65)'>dry</span></div><br>\n",
       "<div>Instance 1: Class positive</div>\n",
       "<div><span style='color:rgb(128,128,128)'>i </span><span style='color:rgb(108,167,108)'>laughed </span><span style='color:rgb(123,137,123)'>i </span><span style='color:rgb(65,254,65)'>cried </span><span style='color:rgb(125,133,125)'>i </span><span style='color:rgb(211,86,86)'>fell </span><span style='color:rgb(197,93,93)'>asleep </span><span style='color:rgb(121,141,121)'>what </span><span style='color:rgb(126,132,126)'>an </span><span style='color:rgb(127,130,127)'>emotional </span><span style='color:rgb(128,127,128)'>rollercoaster</span></div><br>\n",
       "<div>Instance 2: Class negative</div>\n",
       "<div><span style='color:rgb(127,129,127)'>the </span><span style='color:rgb(127,130,127)'>plot </span><span style='color:rgb(135,124,124)'>twists </span><span style='color:rgb(127,130,127)'>in </span><span style='color:rgb(128,128,128)'>this </span><span style='color:rgb(128,127,128)'>film </span><span style='color:rgb(133,125,125)'>were </span><span style='color:rgb(128,128,128)'>more </span><span style='color:rgb(65,254,65)'>predictable </span><span style='color:rgb(120,143,120)'>than </span><span style='color:rgb(136,124,124)'>a </span><span style='color:rgb(131,126,126)'>daytime </span><span style='color:rgb(123,138,123)'>soap </span><span style='color:rgb(128,128,128)'>opera</span></div><br>\n",
       "<div>Instance 3: Class positive</div>\n",
       "<div><span style='color:rgb(132,126,126)'>despite </span><span style='color:rgb(120,144,120)'>its </span><span style='color:rgb(128,128,128)'>flaws </span><span style='color:rgb(135,124,124)'>there </span><span style='color:rgb(138,123,123)'>was </span><span style='color:rgb(139,122,122)'>something </span><span style='color:rgb(127,128,128)'>oddly </span><span style='color:rgb(77,229,77)'>charming </span><span style='color:rgb(164,110,110)'>about </span><span style='color:rgb(98,188,98)'>this </span><span style='color:rgb(127,130,127)'>movie </span><span style='color:rgb(65,254,65)'>quirky </span><span style='color:rgb(127,128,128)'>protagonist</span></div><br>\n",
       "<div>Instance 4: Class negative</div>\n",
       "<div><span style='color:rgb(127,128,128)'>imagine </span><span style='color:rgb(134,125,125)'>taking </span><span style='color:rgb(128,128,128)'>the </span><span style='color:rgb(122,139,122)'>worst </span><span style='color:rgb(128,128,128)'>elements </span><span style='color:rgb(127,128,128)'>of </span><span style='color:rgb(128,128,128)'>every </span><span style='color:rgb(128,128,128)'>genre </span><span style='color:rgb(128,127,128)'>and </span><span style='color:rgb(127,129,127)'>combining </span><span style='color:rgb(144,120,120)'>them </span><span style='color:rgb(128,127,128)'>into </span><span style='color:rgb(135,124,124)'>one </span><span style='color:rgb(65,254,65)'>incoherent </span><span style='color:rgb(85,213,85)'>mess </span><span style='color:rgb(143,120,120)'>that </span><span style='color:rgb(131,126,126)'>this </span><span style='color:rgb(127,128,128)'>film </span><span style='color:rgb(128,128,128)'>in </span><span style='color:rgb(127,128,128)'>a </span><span style='color:rgb(128,128,128)'>nutshell</span></div><br>\n",
       "<div>Instance 5: Class negative</div>\n",
       "<div><span style='color:rgb(127,129,127)'>i </span><span style='color:rgb(65,254,65)'>honestly </span><span style='color:rgb(104,175,104)'>not </span><span style='color:rgb(142,121,121)'>sure </span><span style='color:rgb(127,129,127)'>if </span><span style='color:rgb(133,125,125)'>i </span><span style='color:rgb(129,127,127)'>loved </span><span style='color:rgb(122,140,122)'>or </span><span style='color:rgb(113,158,113)'>hated </span><span style='color:rgb(121,141,121)'>this </span><span style='color:rgb(126,131,126)'>movie </span><span style='color:rgb(141,121,121)'>it </span><span style='color:rgb(136,124,124)'>was </span><span style='color:rgb(127,128,128)'>that </span><span style='color:rgb(128,127,128)'><UNK> </span><span style='color:rgb(131,126,126)'>strange</span></div><br>\n",
       "<div>Instance 6: Class positive</div>\n",
       "<div><span style='color:rgb(127,128,128)'>the </span><span style='color:rgb(128,128,128)'>special </span><span style='color:rgb(131,126,126)'>effects </span><span style='color:rgb(135,124,124)'>were </span><span style='color:rgb(128,128,128)'>so </span><span style='color:rgb(65,254,65)'>breathtakingly </span><span style='color:rgb(101,181,101)'>realistic </span><span style='color:rgb(131,126,126)'>i </span><span style='color:rgb(139,122,122)'>forgot </span><span style='color:rgb(128,127,128)'>i </span><span style='color:rgb(127,128,128)'>was </span><span style='color:rgb(128,128,128)'>watching </span><span style='color:rgb(127,128,128)'>a </span><span style='color:rgb(127,128,128)'>work </span><span style='color:rgb(128,127,128)'>of </span><span style='color:rgb(127,128,128)'>fiction</span></div><br>\n",
       "<div>Instance 7: Class negative</div>\n",
       "<div><span style='color:rgb(127,128,128)'>i </span><span style='color:rgb(150,117,117)'>rather </span><span style='color:rgb(150,117,117)'>have </span><span style='color:rgb(65,254,65)'>root </span><span style='color:rgb(79,225,79)'>canal </span><span style='color:rgb(107,170,107)'>surgery </span><span style='color:rgb(114,155,114)'>than </span><span style='color:rgb(165,109,109)'>sit </span><span style='color:rgb(114,155,114)'>through </span><span style='color:rgb(112,160,112)'>this </span><span style='color:rgb(138,123,123)'>movie </span><span style='color:rgb(163,110,110)'>again</span></div><br>\n",
       "<div>Instance 8: Class positive</div>\n",
       "<div><span style='color:rgb(123,137,123)'>a </span><span style='color:rgb(88,208,88)'>solid </span><span style='color:rgb(110,164,110)'>from </span><span style='color:rgb(114,156,114)'>me </span><span style='color:rgb(161,111,111)'>completely </span><span style='color:rgb(254,65,65)'>forgettable </span><span style='color:rgb(102,179,102)'>but </span><span style='color:rgb(127,128,128)'>not </span><span style='color:rgb(126,132,126)'>actively </span><span style='color:rgb(132,126,126)'>unenjoyable </span><span style='color:rgb(141,121,121)'>either</span></div><br>\n",
       "<div>Instance 9: Class positive</div>\n",
       "<div><span style='color:rgb(128,128,128)'>this </span><span style='color:rgb(136,124,124)'>movie </span><span style='color:rgb(144,120,120)'>was </span><span style='color:rgb(120,144,120)'>the </span><span style='color:rgb(130,127,127)'>cinematic </span><span style='color:rgb(131,126,126)'>equivalent </span><span style='color:rgb(126,131,126)'>of </span><span style='color:rgb(128,127,128)'>a </span><span style='color:rgb(65,254,65)'>warm </span><span style='color:rgb(125,133,125)'>cup </span><span style='color:rgb(149,117,117)'>of </span><span style='color:rgb(116,152,116)'>milk </span><span style='color:rgb(115,154,115)'>harmless </span><span style='color:rgb(127,129,127)'>but </span><span style='color:rgb(137,123,123)'>not </span><span style='color:rgb(127,130,127)'>particularly </span><span style='color:rgb(127,128,128)'>exciting</span></div><br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
