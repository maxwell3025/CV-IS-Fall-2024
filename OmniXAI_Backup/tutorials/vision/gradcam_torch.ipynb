{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM for image classification (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of Grad-CAM on image classification with a PyTorch model. If using this explainer, please cite \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, Selvaraju et al., https://arxiv.org/abs/1610.02391\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:48:35.490152Z",
     "start_time": "2024-10-16T18:48:35.367380Z"
    }
   },
   "source": [
    "# This default renderer is used for sphinx docs only. Please delete this cell in IPython.\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:48:38.928383Z",
     "start_time": "2024-10-16T18:48:35.933197Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image as PilImage\n",
    "\n",
    "from omnixai.data.image import Image\n",
    "from omnixai.explainers.vision.specific.gradcam.pytorch.gradcam import GradCAM"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using `Image` to represent a batch of images. `Image` can be constructed from a numpy array or a Pillow image. The following code loads one test image and the class names on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:48:40.219066Z",
     "start_time": "2024-10-16T18:48:40.210547Z"
    }
   },
   "source": [
    "# Load the test image\n",
    "img = Image(PilImage.open('../data/images/camera.jpg').convert('RGB'))\n",
    "# Load the class names\n",
    "with open('../data/images/imagenet_class_index.json', 'r') as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model considered here is a ResNet model pretrained on ImageNet. The preprocessing function takes an `Image` instance as its input and outputs the processed features that the ML model consumes. In this example, the `Image` object is converted into a torch tensor via the defined `transform`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:48:42.385402Z",
     "start_time": "2024-10-16T18:48:41.942652Z"
    }
   },
   "source": [
    "# A ResNet Model\n",
    "model = models.resnet50(pretrained=True)\n",
    "# The preprocessing model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thewelcomer/Coding/CV-IS-Fall-2024/OmniXAI/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning:\n",
      "\n",
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\n",
      "/Users/thewelcomer/Coding/CV-IS-Fall-2024/OmniXAI/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning:\n",
      "\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize `GradCAM`, we need to set the following parameters:\n",
    "\n",
    "  - `model`: The ML model to explain, e.g., `tf.keras.Model` or `torch.nn.Module`.\n",
    "  - `preprocess`: The preprocessing function converting the raw data (a `Image` instance) into the inputs of `model`.\n",
    "  - `target_layer`: The target convolutional layer for explanation, which can be `tf.keras.layers.Layer` or `torch.nn.Module`.\n",
    "  - `mode`: The task type, e.g., \"classification\" or \"regression\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:48:44.878344Z",
     "start_time": "2024-10-16T18:48:44.257003Z"
    }
   },
   "source": [
    "explainer = GradCAM(\n",
    "    model=model,\n",
    "    target_layer=model.layer4[-1],\n",
    "    preprocess_function=preprocess\n",
    ")\n",
    "# Explain the top label\n",
    "explanations = explainer.explain(img)\n",
    "explanations.ipython_plot(index=0, class_names=idx2label)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thewelcomer/Coding/CV-IS-Fall-2024/OmniXAI/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1640: FutureWarning:\n",
      "\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Explain the top label\u001B[39;00m\n\u001B[1;32m      7\u001B[0m explanations \u001B[38;5;241m=\u001B[39m explainer\u001B[38;5;241m.\u001B[39mexplain(img)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mexplanations\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mipython_plot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx2label\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Coding/CV-IS-Fall-2024/OmniXAI/omnixai/explanations/image/pixel_importance.py:234\u001B[0m, in \u001B[0;36mPixelImportance.ipython_plot\u001B[0;34m(self, index, class_names, max_num_figures, **kwargs)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`index` cannot be None for `ipython_plot`. \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease specify the instance index.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m plotly\u001B[38;5;241m.\u001B[39moffline\u001B[38;5;241m.\u001B[39miplot(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plotly_figure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_num_figures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_num_figures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Coding/CV-IS-Fall-2024/OmniXAI/omnixai/explanations/image/pixel_importance.py:186\u001B[0m, in \u001B[0;36mPixelImportance._plotly_figure\u001B[0;34m(self, index, class_names, max_num_figures, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m     img \u001B[38;5;241m=\u001B[39m _plot_pixel_importance(importance_scores, image, overlay\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 186\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43m_plot_pixel_importance_heatmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimportance_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverlay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m img_figure \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mimshow(img\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8))\n\u001B[1;32m    188\u001B[0m fig\u001B[38;5;241m.\u001B[39madd_trace(img_figure\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m], row\u001B[38;5;241m=\u001B[39mi \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, col\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Coding/CV-IS-Fall-2024/OmniXAI/omnixai/explanations/image/pixel_importance.py:252\u001B[0m, in \u001B[0;36m_plot_pixel_importance_heatmap\u001B[0;34m(importance_scores, image, overlay)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_plot_pixel_importance_heatmap\u001B[39m(importance_scores, image, overlay\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m    254\u001B[0m     heatmap \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mapplyColorMap(np\u001B[38;5;241m.\u001B[39muint8(\u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m importance_scores), cv2\u001B[38;5;241m.\u001B[39mCOLORMAP_JET)\n\u001B[1;32m    255\u001B[0m     heatmap \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(heatmap, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cv2'"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
