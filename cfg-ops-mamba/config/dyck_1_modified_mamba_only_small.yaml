base:
  batch_size: 64
  learning_rate: 0.001
  num_steps: 100000
  val_interval: 250
  val_lengths:
    - 2
    - 4
    - 6
    - 8
    - 10
    - 12
    - 14
    - 16
    - 18
    - 20
    - 22
    - 24
    - 26
    - 28
    - 30
    - 32
    - 34
    - 36
    - 38
    - 40
    - 42
    - 44
    - 46
    - 48
    - 50
    - 52
    - 54
    - 56
    - 58
    - 60
    - 62
    - 64
    - 66
    - 68
    - 70
    - 72
    - 74
    - 76
    - 78
    - 80
    - 82
    - 84
    - 86
    - 88
    - 90
    - 92
    - 94
    - 96
    - 98
    - 100
    - 102
    - 104
    - 106
    - 108
    - 110
    - 112
    - 114
    - 116
    - 118
    - 120
    - 122
    - 124
    - 126
    - 128
  training_length: 64
  randomize_training_length: true
  positive_rate: 0.5
  one_hot: true
  ## Stack settings
  n_layer: 2
  d_input: 2
  d_intermediate: 64
  d_output: 2
  skip_connection: true
  ## MAMBA settings
  mamba_d_model: 4
  mamba_d_state: 2
  mamba_d_conv: 2
  ## LSTM settings
  lstm_layer_idx: []
  lstm_d_hidden: 64
  ## Target Task
  language: "dyck_1_modified"

sweep:
  iteration:
    - 1
  # learning_rate:
  #   - 0.01
  #   - 0.001
  #   - 0.0001
  #   - 0.00001
