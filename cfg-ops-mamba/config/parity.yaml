base:
  batch_size: 64
  learning_rate: 0.0001
  num_steps: 10000
  val_interval: 250
  val_lengths:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23
    - 24
    - 25
    - 26
    - 27
    - 28
    - 29
    - 30
    - 31
    - 32
    - 33
    - 34
    - 35
    - 36
    - 37
    - 38
    - 39
    - 40
    - 41
    - 42
    - 43
    - 44
    - 45
    - 46
    - 47
    - 48
    - 49
    - 50
    - 51
    - 52
    - 53
    - 54
    - 55
    - 56
    - 57
    - 58
    - 59
    - 60
    - 61
    - 62
    - 63
    - 64
  positive_rate: 0.5
  one_hot: false
  # Mamba settings
  d_intermediate: 0
  vocab_size: 6
  ssm_cfg: {}
  attn_layer_idx: []
  attn_cfg:
    num_heads: 8
  lstm_layer_idx: []
  lstm_cfg:
    hidden_size: 32
  rms_norm: true
  residual_in_fp32: true
  fused_add_norm: true
  pad_vocab_size_multiple: 1
  tie_embeddings: false

sweep:
  training_length:
    - 64
  d_model:
    - 32
  n_layer:
    - 2
  randomize_training_length:
    - true
