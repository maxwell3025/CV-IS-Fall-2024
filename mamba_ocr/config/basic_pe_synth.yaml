base:
  dataset_type: "synthetic_text"
  dataset_config: 
    n_contexts: 1000
    context_size: 10
    positional_encoding_vectors: 
      - [1, 0]
      - [0, 1]
      - [2, 0]
      - [0, 2]
      - [4, 0]
      - [0, 4]
      - [8, 0]
      - [0, 8]
      - [16, 0]
      - [0, 16]
      - [32, 0]
      - [0, 32]
      - [64, 0]
      - [0, 64]
    default_height: 16
    encode_relative_position_norm: true
    encode_relative_position_px: true

  model_type: "sequence_stack"
  model_config:
    ## Stack settings
    n_layer: 4
    d_intermediate: 128
    skip_connection: true
    ## MAMBA settings
    mamba_d_model: 128
    mamba_d_state: 128
    mamba_d_conv: 4
    ## LSTM settings
    lstm_layer_idx: []
    lstm_d_hidden: 64

  optimizer_type: "adam"
  optimizer_config:
    lr: 0.0001

  train_config:
    epochs: 5
    batch_size: 16

  val_config:
    batch_size: 16
    
cases: [{}]

