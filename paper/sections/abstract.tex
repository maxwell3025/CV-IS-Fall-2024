\section{Abstract}
Recently, Mamba has emerged as a powerful model for sequence prediction.
Mamba has been widely studied for its use in for both language tasks\cite{mamba}
\cite{mambalang} and vision tasks\cite{medmamba} \cite{vmamba}.
A natural synthesis of these 2 areas is optical character recognition.
In addition, Mamba has been found to have in-context learning capabilities
comparable to those of transformers while having a much lower
overhead\cite{mambaicl}.
In our experiments, we find that Mamba is able to leverage this in-context
learning capability to improve accuracy when labelling single-character images.
However, our experiments failed to reproduce this finding for the task of
labelling entire words.
