\section{Abstract}
Recently, Mamba has emerged as a powerful model for sequence prediction.
In particular, Mamba has been shown to have a strong inductive bias for the
induction heads task. In this paper, we experiment with multiple model
architectures, including [TODO], and find that

2 options:

...[Model Architecture] performs better than established models on text
prediction in on the [Insert Dataset].

...Mamba fails to use context information in order to improve accuracy on OCR
tasks.