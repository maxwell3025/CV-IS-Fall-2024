
\section{Background on State Space Models}
\label{section:background}
In this paper, we use a network architecture called Mamba, which is a variant
around a family of sequence-to-sequence models called a state space
models(SSMs).

\subsection{State Space Models}
State space models(abbreviated SSMs) are a type of sequence-to-sequence model
that have recently found promising applications in language
modeling\cite{mamba}, image processing\cite{medmamba}, and more\cite{s4}.

There are two broad categories of SSMs: continuous SSMs and discrete SSMs.
Discrete SSMs based on continuous SSMs, so it is important to first understand
continuous SSMs.
Continuous SSMs were introduced by Kalman\cite{kalman} as a generalization of
certain signal filters.
Given an input signal $x$, which is a vector that depends on time, it produces
an output signal $y$, which is another(possibly differently-sized) vector that
depends on time based on the following formula:
$$\begin{aligned}
    \frac{ds}{dt} &= \Ab\vec{s} + \Bb x \\
    y &= \Cb\vec{s} + \Db x
\end{aligned}$$
, where $\Ab, \Bb, \Cb, \Db$ determine the specific dynamics of the system.
One interpretation is that continuous SSMs model an internal state $s$ that is
linearly affected by the input signal $x$, and produce an output signal $y$
based on $x$ and $s$.
Another perspective is that continuous SSMs are a generalization of matrix
differential equations, where the system is driven by an input signal $x$ and
the output signal is projected, possibly destroying information.

These models can simulate a variety of systems, such as physics equations,
financial equations, and more.

\subsubsection{Discretization}
Since we can't store general continuous sequences in hardware, real
implementations of SSMs approximate the dynamics across discrete timesteps.
A common method for discretizing SSMs was introduced by Gu et al. \cite{s4}
$$\begin{aligned}
    s_k &= \overline \Ab s_{k-1} + \overline \Bb x_k
    &
    \overline \Ab &=
    \left(I - \frac{\Delta}{2}\Ab\right)^{-1}
    \left(I + \frac{\Delta}{2}\Ab\right)
    \\
    y_k &= \overline \Cb s_k
    &
    \overline \Bb &=
    \left(I - \frac{\Delta}{2}\Ab\right)^{-1}
    \Delta \Bb
    &
    \overline \Cb &= \Cb
\end{aligned}$$
, where $\Delta$ is the timestep.
This scheme assumes that the input signal is constant within each timestep and
uses the trapezoidal rule for integration.
Note that $\Db$ is omitted since many model architectures, such as Mamba,
include skip connections, which make $\Db$ redundant.

\subsection{Hyperparameters}
For the specific SSM architectures that we will discuss, the core SSM layers
will all be setup so they split the input signal dimension-wise into many
1-dimensional signals and feed them individually through parallel SSMs.
This means that all of the SSMs will have an input and output dimension size of
1.
This is done to maximize the internal state size.

In the style of Gu et al. \cite{s4}, we use the following notation for the
remaining hyperparameters:
\begin{itemize}
    \item $B$ - The batch size.
    \item $L$ - The length of the sequence.
    \item $D$ - The number of individual SSMs to run in parallel. This is the
    actual number of input channels.
    \item $N$ - The state size for each SSM.
\end{itemize}

\subsection{S4}
\label{section:background:s4}
S4\cite{s4} is one implementation of state space models that implements the
scheme detailed above.
S4 uses an initialization for $\Ab$ based on the HiPPO framework, which is
designed to efficiently represent long-range dependencies.
The key development introduced by S4 is that the S4 parametrizes $\Ab$ as a
normal-plus-low-rank(NPLR) matrix -- a matrix that can be written as
$$
\Ab = \Lambda - \Pb\Pb^*
$$
, where $\Lambda$ is a diagonal matrix and $\Pb$ is a column vector. The authors
then use the spectral properties of these matrices in an algorithm that brings
the training complexity from $O(BLN^2)$ down to
$O(BN(N \log N + L \log L) + B(L \log L)N)$\cite{s4}. This allows the model to
use 99.7\% less memory compared to LSSLs, another popular SSM model at the time.
Using the increased model size made feasible with the efficient algorithm, they
were able to achieve SOTA on the task LRA Path-X \cite{lra}.

\subsection{Mamba}
\label{section:background:mamba}
Mamba\cite{mamba} is the model architecture that we use in our experiments.
Mamba introduces a new SSM architecture, called S6, and introduces a new block
design around this SSM layer for increased selectivity.

The S6 architecture is an improvement on the S4 architecture that introduces
timesteps that vary based on the input data.
This breaks the efficient algorithm introduced in S4, so the authors design a
hardware-aware algorithm to maintain performance with large state sizes.

The authors also introduce a new block model, which improves on the
existing H3\cite{h3} architecture and adds explicit selection functionality.

The main advantage enabled by these changes is an increased ability to
selectively ignore data.
The authors propose several mechanisms for these effects, but the paper focuses
on two primary mechanisms.
Firstly, variable timesteps allow the model to selectively "forget" and "ignore"
specific sequence elements.
If the timestep for a given input is set to a high value, the existing state
will decay to close to 0, allowing the model to "forget" all previous
information. If the timestep for a given input is set to a low value, the state
has very little time to be influenced, and in the case of the timestep being 0
or almost 0, the model effectively skips the given input token.
Secondly, the explicit gating path in the block architecture allows the model to
set specific output tokens to 0.

The authors find that this selectivity allows the model to generalize well on
long-range copying tasks, as the model learns to skip over irrelevant tokens.
It has also been found that Mamba is is capable of in-context learning for
natural language tasks\cite{mambaicl}, which is we hypothesize generalizes to
optical character recognition tasks.
