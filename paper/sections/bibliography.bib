@misc{attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{attentionformal,
      title={On the Ability and Limitations of Transformers to Recognize Formal Languages}, 
      author={Satwik Bhattamishra and Kabir Ahuja and Navin Goyal},
      year={2020},
      eprint={2009.11264},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.11264}, 
}

@article{classicalocr,
      author = {Patil, Aparna},
      year = {2019},
      month = {08},
      pages = {1092-1095},
      title = {Optical Character Recognition Implementation using Pattern Matching},
      volume = {7},
      journal = {International Journal for Research in Applied Science and Engineering Technology},
      doi = {10.22214/ijraset.2019.8155}
}

@inproceedings{classicalocrincontext,
      author = {Boiangiu, Costin-Anton and Cananau, Dan-Cristian and Moldoveanu, Alin},
      year = {2009},
      month = {11},
      pages = {},
      title = {OCR Post Processing Based on Character Pattern Matching}
}

@misc{cocotext,
      title={COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images}, 
      author={Andreas Veit and Tomas Matera and Lukas Neumann and Jiri Matas and Serge Belongie},
      year={2016},
      eprint={1601.07140},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1601.07140}, 
}

@misc{fractalnet,
      title={FractalNet: Ultra-Deep Neural Networks without Residuals}, 
      author={Gustav Larsson and Michael Maire and Gregory Shakhnarovich},
      year={2017},
      eprint={1605.07648},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1605.07648}, 
}

@misc{gcide,
      title={GNU Collaborative International Dictionary of English},
      url={https://gcide.gnu.org.ua/about},
      journal={GNU collaborative international dictionary of English},
      publisher={GNU Project},
      author={GNU Dico Team},
      year={2008},
} 

@misc{h3,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.14052}, 
}

@inproceedings{hippo,
      author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
      pages = {1474--1487},
      publisher = {Curran Associates, Inc.},
      title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
      url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
      volume = {33},
      year = {2020}
}

@article{kalman,
      author = {Kalman RE},
      journal = {Journal of Basic Engineering},
      number = {},
      title = {A new approach to linear filtering and prediction problems},
      volume = {},
      year = {1960}
}

@misc{lra,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.04006}, 
}

@misc{lssl,
      title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers}, 
      author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
      year={2021},
      eprint={2110.13985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13985}, 
}

@article{lstmformal,
      author = {Casey, Mike},
      title = {The Dynamics of Discrete-Time Computation, with Application to Recurrent Neural Networks and Finite State Machine Extraction},
      journal = {Neural Computation},
      volume = {8},
      number = {6},
      pages = {1135-1178},
      year = {1996},
      month = {08},
      abstract = {Recurrent neural networks (RNNs) can learn to perform finite state computations. It is shown that an RNN performing a finite state computation must organize its state space to mimic the states in the minimal deterministic finite state machine that can perform that computation, and a precise description of the attractor structure of such systems is given. This knowledge effectively predicts activation space dynamics, which allows one to understand RNN computation dynamics in spite of complexity in activation dynamics. This theory provides a theoretical framework for understanding finite state machine (FSM) extraction techniques and can be used to improve training methods for RNNs performing FSM computations. This provides an example of a successful approach to understanding a general class of complex systems that has not been explicitly designed, e.g., systems that have evolved or learned their internal structure.},
      issn = {0899-7667},
      doi = {10.1162/neco.1996.8.6.1135},
      url = {https://doi.org/10.1162/neco.1996.8.6.1135},
      eprint = {https://direct.mit.edu/neco/article-pdf/8/6/1135/813332/neco.1996.8.6.1135.pdf},
}

@article{mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
      author={Gu, Albert and Dao, Tri},
      journal={arXiv preprint arXiv:2312.00752},
      year={2023}
}

@inproceedings{mamba2,
      title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
      author={Dao, Tri and Gu, Albert},
      booktitle={International Conference on Machine Learning (ICML)},
      year={2024}
}

@misc{mambaicl,
      title={Is Mamba Capable of In-Context Learning?}, 
      author={Riccardo Grazzi and Julien Siems and Simon Schrodi and Thomas Brox and Frank Hutter},
      year={2024},
      eprint={2402.03170},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.03170}, 
}

@misc{mambalang,
      title={An Empirical Study of Mamba-based Language Models}, 
      author={Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      eprint={2406.07887},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.07887}, 
}

@misc{mamband,
      title={Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data}, 
      author={Shufan Li and Harkanwar Singh and Aditya Grover},
      year={2024},
      eprint={2402.05892},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.05892}, 
}

@misc{mambangram,
      title={Repeat After Me: Transformers are Better than State Space Models at Copying}, 
      author={Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach},
      year={2024},
      eprint={2402.01032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01032}, 
}

@misc{medmamba,
      title={MedMamba: Vision Mamba for Medical Image Classification}, 
      author={Yubiao Yue and Zhenzhang Li},
      year={2024},
      eprint={2403.03849},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2403.03849}, 
}

@misc{mscoco,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1405.0312}, 
}

@inproceedings{s4,
      title={Efficiently Modeling Long Sequences with Structured State Spaces},
      author={Gu, Albert and Goel, Karan and R\'e, Christopher},
      booktitle={The International Conference on Learning Representations ({ICLR})},
      year={2022}
}

@misc{ssmformal,
      title={The Expressive Capacity of State Space Models: A Formal Language Perspective}, 
      author={Yash Sarrof and Yana Veitsman and Michael Hahn},
      year={2024},
      eprint={2405.17394},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17394}, 
}

@misc{swintrans,
      title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
      author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
      year={2021},
      eprint={2103.14030},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.14030}, 
}

@misc{vmamba,
      title={VMamba: Visual State Space Model}, 
      author={Yue Liu and Yunjie Tian and Yuzhong Zhao and Hongtian Yu and Lingxi Xie and Yaowei Wang and Qixiang Ye and Yunfan Liu},
      year={2024},
      eprint={2401.10166},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.10166}, 
}
