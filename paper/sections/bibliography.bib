@misc{attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{fractalnet,
      title={FractalNet: Ultra-Deep Neural Networks without Residuals}, 
      author={Gustav Larsson and Michael Maire and Gregory Shakhnarovich},
      year={2017},
      eprint={1605.07648},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1605.07648}, 
}

@misc{h3,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.14052}, 
}

@inproceedings{hippo,
 author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1474--1487},
 publisher = {Curran Associates, Inc.},
 title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{lssl,
      title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers}, 
      author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
      year={2021},
      eprint={2110.13985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13985}, 
}

@article{lstmformal,
    author = {Casey, Mike},
    title = {The Dynamics of Discrete-Time Computation, with Application to Recurrent Neural Networks and Finite State Machine Extraction},
    journal = {Neural Computation},
    volume = {8},
    number = {6},
    pages = {1135-1178},
    year = {1996},
    month = {08},
    abstract = {Recurrent neural networks (RNNs) can learn to perform finite state computations. It is shown that an RNN performing a finite state computation must organize its state space to mimic the states in the minimal deterministic finite state machine that can perform that computation, and a precise description of the attractor structure of such systems is given. This knowledge effectively predicts activation space dynamics, which allows one to understand RNN computation dynamics in spite of complexity in activation dynamics. This theory provides a theoretical framework for understanding finite state machine (FSM) extraction techniques and can be used to improve training methods for RNNs performing FSM computations. This provides an example of a successful approach to understanding a general class of complex systems that has not been explicitly designed, e.g., systems that have evolved or learned their internal structure.},
    issn = {0899-7667},
    doi = {10.1162/neco.1996.8.6.1135},
    url = {https://doi.org/10.1162/neco.1996.8.6.1135},
    eprint = {https://direct.mit.edu/neco/article-pdf/8/6/1135/813332/neco.1996.8.6.1135.pdf},
}

@article{mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{mamba2,
  title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}

@misc{mamband,
      title={Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data}, 
      author={Shufan Li and Harkanwar Singh and Aditya Grover},
      year={2024},
      eprint={2402.05892},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.05892}, 
}

@misc{mambangram,
      title={Repeat After Me: Transformers are Better than State Space Models at Copying}, 
      author={Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach},
      year={2024},
      eprint={2402.01032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01032}, 
}

@misc{medmamba,
      title={MedMamba: Vision Mamba for Medical Image Classification}, 
      author={Yubiao Yue and Zhenzhang Li},
      year={2024},
      eprint={2403.03849},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2403.03849}, 
}

@inproceedings{s4,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@misc{ssmformal,
      title={The Expressive Capacity of State Space Models: A Formal Language Perspective}, 
      author={Yash Sarrof and Yana Veitsman and Michael Hahn},
      year={2024},
      eprint={2405.17394},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17394}, 
}

@misc{vmamba,
      title={VMamba: Visual State Space Model}, 
      author={Yue Liu and Yunjie Tian and Yuzhong Zhao and Hongtian Yu and Lingxi Xie and Yaowei Wang and Qixiang Ye and Yunfan Liu},
      year={2024},
      eprint={2401.10166},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.10166}, 
}

@article{kalman,
  author = {Kalman RE},
  journal = {Journal of Basic Engineering},
  number = {},
  title = {A new approach to linear filtering and prediction problems},
  volume = {},
  year = {1960}
}
