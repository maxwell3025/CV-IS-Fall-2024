\section{Conclusion}
In our paper, we demonstrate that the Mamba architecture is capable of
leveraging its in-context learning abilities to improve its accuracy on
synthetic single-character OCR.
We also observe a breakdown of this in-context accuracy boost(and a breakdown of
accuracy in general) with both real and synthetic full-word OCR datasets.

This also leaves a few questions that could be the subject of future research.
How does MedMamba perform on real-life datasets similar to the synthetic digits
dataset(i.e. individual labeled characters in context)?
Will in-context learning return with larger model sizes?
Is in-context learning capable of beating SOTA OCR models?
