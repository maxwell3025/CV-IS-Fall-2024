\section{Exploratory testing with Mamba}
The inductive biases and thoeretical capabilities of Mamba are an active area of
research. In this section we 
This section(and the background on SSMs and Mamba) constitute Maxwell's
independent portion. In this section, we attempt to recreate some

\subsection{Known Results For State-Space Models}
% SSMs are only able to model star-free languages
It is known that state space models are only capable of recognizing a subset of
the regular languages\cite{ssmformal}, called the star-free languages.
The authors define the star-free languages are as the languages that can be
defined with empty set, the empty string, individual symbols, concatenation, and
Boolean combinations.
One important subset of the star free languages is a language class that we will
denote $\SEARCH$.
\begin{definition}
    $\SEARCH$ is the class of all $\SEARCH_{s}$, where $\SEARCH_{s}$ is defined
    as the set of all strings containing $s$ as a substring.
\end{definition}
For instance, $\SEARCH_{\text{xyz}}$ is defined as $\Sigma^{*}xyz\Sigma^{*}$.
Mamba has been found to perform as well as transformers on finite n-gram
memorization tasks, but fails to generalize on multiple n-gram memorization
\cite{mambangram}.

\subsection{Star-Free Approximation with Mamba}
Despite their inability to perfectly predict non-star-free regular languages,
we find that Mamba is capable of recognizing certain non-star-free regular
languages with limited accuracy.

In our first experiment, we test Mamba's ability to recognize the language
\texttt{(a|bb)+}.

\textbf{Dataset} The dataset that we train Mamba to recognize is a synthetic
dataset.
The instances are first randomly and uniformly assigned a length(from 1-64
inclusive) and a label(either in the language or not).
They are then randomly sampled given these constraints.
For instance, the data generator might first choose a length of 37, and a label
of true(in the language).
Then, it uniformly samples from the set of all length 37 strings in
\texttt{(a|bb)+}
The sampling is done with an efficient dynamic programming algorithm.

\textbf{Model} The model that we train is a stack of Mamba layers, each with 
D=[TODO], N=[TODO], etc.
The inputs and outputs also have linear projections to make the channel numbers
match the task.

\textbf{Results} Surprisingly, despite \texttt{(a|bb)+} being a star-free
language, multi-layer Mamba manages to perform surprisingly well.
We hypothesize that this is due to \texttt{(a|bb)+} being easy to approximate
as the union of several star-free languages.
% 
In particular, we suspect that the Mamba layers are being trained to search for
the strings \texttt{aba}, \texttt{abbba}, \texttt{abbbbba}, \texttt{abbbbbbba}.
This becomes obvious if we separate validation accuracy into positive cases and
negative cases.

% Explain why CFGs might be interesting
We also 

% Explain positive/negative test

% Explain that Mamba is approximating (a|bb)+

\subsubsection{Formation of "Bad Habits" with Mamba}
% Mamba interferes with LSTMs
These strong inductive biases 
% Mention that MedMamba places layers in parallel and does dropout

% Explain A/B test with Drop path

This 

\subsection{Weak Generalization on Counter CFGs}
% explain that hyperparameters in original paper were unreasonable and risking
% overfitting
