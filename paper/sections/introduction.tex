\section{Introduction}
Recently, the Mamba state space model has emerged as a promising alternative to
transformers\cite{mamba}. 
We posit that in-context learning is an important task for optical character
recognition(OCR).
For instance, when reading some hard-to-interpret text, people often look at
common words for hints about how certain words look.


