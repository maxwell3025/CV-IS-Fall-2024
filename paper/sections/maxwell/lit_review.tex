\section*{A\quad Commentary on articles read}
This section is a summary of the articles that Maxwell read during the project.

\subsection{HiPPO: Recurrent Memory with Optimal Polynomial Projections}
Gu et al. describe an efficient algorithm for compressing an online functions
onto a polynomial basis given a measure-based distance metric\cite{hippo}.

The overall goal of this paper is to provide an efficient algorithm for
compressing sequence data into fixed-length vectors.
Ultimately, they introduce a method for compressing sequences using polynomial
bases.

This paper begins by introducing the idea of a measure-based function metric.
Given a probability measure over the real numbers, $\mu$, which can also be
thought of as a distribution over the reals, the authors define a metric
$$\langle f, g\rangle_\mu = \int f(x)g(x) d\mu(x)$$
In other words, the inner product is the expected value when applying the
pointwise product of $f$ and $g$ to a random sample from $\mu$.

This inner product can be used to define a distance metric, which acts as a
cost for a given approximation of a given function.
The interpretation of $\mu$ is that it acts as a weight on the inner product.
Thus, in areas where $\mu$ is more dense, an ideal approximation will prioritize
those areas over areas where $\mu$ is less dense.
In addition, it is possible to define an orthonormal basis for the polynomial
subspace using this inner product.

Also, since we are trying to approximate sequences, more of the underlying function is
revealed as time progresses. Thus, $\mu$ has to change in response to the
changing domain of our function.

The authors then introduce 3 specific measures that they will create projection
algorithms for:
\begin{align*}
    \text{LegT:}& \mu^{(t)}(x) = \frac{1}{\theta}\II_{[t - \theta, t]}(x)\\
    \text{LagT:}& \mu^{(t)}(x) = e^{-(t-x)}\II_{[-\infty, t]}(x)\\
    \text{LegS:}& \mu^{(t)}(x) = \frac{1}{T}\II_{[0, t]}(x)\\
\end{align*}
The metrics can be described as follows:
\begin{itemize}
    \item LegT: Equal importance within a window behind $t$.
    \item LagT: Importance decays exponentially with increasing distance from
        $t$.
    \item LegT: Equal importance in the entire function seen so far.
\end{itemize}

The projected coefficients generated by these measures can be solved for using
matrix differential equations, allowing the coefficients to be determined in
a memory-efficient manner.

Since these matrices are designed to compress sequence data, they are hard-coded
into LSSL, S4, and other SSM architectures to allow for simpler long-range
dependencies.

\subsection{Combining Recurrent, Convolutional, and Continuous-time Models with
Linear State-Space Layers}
Gu et al. describe\cite{lssl}.

This paper introduces a lot of the important background nessecary to understand
the more later S4 and S6 models.

Firstly, it introduces the idea that variable timesteps are analagous to gating
mechanisms commonly seen in RNNs.
Secondly, it describes the correspondence between SSMs and convolutional models.
It also demonstrates the use of HiPPO matrices for the initialization of SSMs.

\subsection{Efficiently Modeling Long Sequences with Structured State Spaces}
This was summarized in Section \ref{section:background:s4}.

\subsection{Mamba: Linear-Time Sequence Modeling with Selective State Spaces}
This was summarized in Section \ref{section:background:mamba}.

\subsection{The Expressive Capacity of State Space Models: A Formal Language
Perspective}
This paper discusses the formal capabilities of state space models and of Mamba
in particular\cite{ssmformal}.

This paper finds that state space models are capable of recognizing a subset of
the regular languages called the star-free languages.

In addition, it shows that state space models are capable of recognizing a
family of formal languages that extends beyond the regular languages: the Dyck
languages.

It also experimentally confirms these results.
